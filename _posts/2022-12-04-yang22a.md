---
section: Contributed Papers
title: Regularizing Brain Age Prediction via Gated Knowledge Distillation
abstract: The brain age has been proven a phenotype with relevance to cognitive performance
  and brain disease. With the development of deep learning, brain age estimation accuracy
  has been greatly improved. However, such methods may incur over-fitting and suffer
  from poor generalizations, especially for insufficient brain imaging data. This
  paper presents a novel regularization method that penalizes the predictive distribution
  using knowledge distillation and introduces additional knowledge to reinforce the
  learning process. During knowledge distillation, we propose a gated distillation
  mechanism to enable the student model to attentively learn key knowledge from the
  teacher model, given the assumption that the teacher may not always be correct.
  Moreover, to enhance the capability of knowledge transfer, the hint representation
  similarity is also adopted to regularize the model training. We evaluate the model
  by a cohort of 3655 subjects from 4 public datasets, demonstrating that the proposed
  method improves the prediction performance over several well-established models,
  where the mean absolute error of the estimated ages is 2.129 years.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang22a
month: 0
tex_title: Regularizing Brain Age Prediction via Gated Knowledge Distillation
firstpage: 1430
lastpage: 1443
page: 1430-1443
order: 1430
cycles: false
bibtex_author: Yang, Yanwu and Xutao, Guo and Ye, Chenfei and Xiang, Yang and Ma,
  Ting
author:
- given: Yanwu
  family: Yang
- given: Guo
  family: Xutao
- given: Chenfei
  family: Ye
- given: Yang
  family: Xiang
- given: Ting
  family: Ma
date: 2022-12-04
address:
container-title: Proceedings of The 5th International Conference on Medical Imaging
  with Deep Learning
volume: '172'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 12
  - 4
pdf: https://proceedings.mlr.press/v172/yang22a/yang22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
